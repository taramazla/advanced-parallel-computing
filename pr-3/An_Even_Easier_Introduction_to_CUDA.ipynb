{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WkOA4mcN7Hj"
      },
      "source": [
        "# An Even Easier Introduction to CUDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuOcUi0fvogW"
      },
      "source": [
        "This notebook accompanies Mark Harris's popular blog post [_An Even Easier Introduction to CUDA_](https://developer.nvidia.com/blog/even-easier-introduction-cuda/).\n",
        "\n",
        "If you enjoy this notebook and want to learn more, the [NVIDIA DLI](https://nvidia.com/dli) offers several in depth CUDA Programming courses.\n",
        "\n",
        "For those of you just starting out, please consider [_Fundamentals of Accelerated Computing with CUDA C/C++_](https://courses.nvidia.com/courses/course-v1:DLI+C-AC-01+V1/about) which provides dedicated GPU resources, a more sophisticated programming environment, use of the [NVIDIA Nsight Systems™](https://developer.nvidia.com/nsight-systems) visual profiler, dozens of interactive exercises, detailed presentations, over 8 hours of material, and the ability to earn a DLI Certificate of Competency.\n",
        "\n",
        "Similarly, for Python programmers, please consider [_Fundamentals of Accelerated Computing with CUDA Python_](https://courses.nvidia.com/courses/course-v1:DLI+C-AC-02+V1/about).\n",
        "\n",
        "For more intermediate and advance CUDA programming materials, please check out the _Accelerated Computing_ section of the NVIDIA DLI [self-paced catalog](https://www.nvidia.com/en-us/training/online/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1C6GK_MO5er"
      },
      "source": [
        "<img src=\"https://developer.download.nvidia.com/training/courses/T-AC-01-V1/CUDA_Cube_1K.jpeg\" width=\"400\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcmbR8lZPLRv"
      },
      "source": [
        "This post is a super simple introduction to CUDA, the popular parallel computing platform and programming model from NVIDIA. I wrote a previous [“Easy Introduction”](https://developer.nvidia.com/blog/easy-introduction-cuda-c-and-c/) to CUDA in 2013 that has been very popular over the years. But CUDA programming has gotten easier, and GPUs have gotten much faster, so it’s time for an updated (and even easier) introduction.\n",
        "\n",
        "CUDA C++ is just one of the ways you can create massively parallel applications with CUDA. It lets you use the powerful C++ programming language to develop high performance algorithms accelerated by thousands of parallel threads running on GPUs. Many developers have accelerated their computation- and bandwidth-hungry applications this way, including the libraries and frameworks that underpin the ongoing revolution in artificial intelligence known as [Deep Learning](https://developer.nvidia.com/deep-learning).\n",
        "\n",
        "So, you’ve heard about CUDA and you are interested in learning how to use it in your own applications. If you are a C or C++ programmer, this blog post should give you a good start. To follow along, you’ll need a computer with an CUDA-capable GPU (Windows, Mac, or Linux, and any NVIDIA GPU should do), or a cloud instance with GPUs (AWS, Azure, IBM SoftLayer, and other cloud service providers have them). You’ll also need the free [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit) installed.\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDQ9ycz0Qfyf"
      },
      "source": [
        "<img src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2017/01/cuda_ai_cube-625x625.jpg\" width=\"400\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wH9Rfms_QtXF"
      },
      "source": [
        "## Starting Simple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5-iUihBQvQt"
      },
      "source": [
        "We’ll start with a simple C++ program that adds the elements of two arrays with a million elements each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nc-gBqLDQ7AC",
        "outputId": "6a1b514d-ca75-4331-a929-df021b0ed608"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting add.cpp\n"
          ]
        }
      ],
      "source": [
        "%%writefile add.cpp\n",
        "\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "\n",
        "// function to add the elements of two arrays\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "  for (int i = 0; i < n; i++)\n",
        "      y[i] = x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  int N = 1<<20; // 1M elements\n",
        "\n",
        "  float *x = new float[N];\n",
        "  float *y = new float[N];\n",
        "\n",
        "  // initialize x and y arrays on the host\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        "\n",
        "  // Run kernel on 1M elements on the CPU\n",
        "  add(N, x, y);\n",
        "\n",
        "  // Check for errors (all values should be 3.0f)\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
        "  std::cout << \"Max error: \" << maxError << std::endl;\n",
        "\n",
        "  // Free memory\n",
        "  delete [] x;\n",
        "  delete [] y;\n",
        "\n",
        "  return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gw6DsX4uRHMg"
      },
      "source": [
        "Executing the above cell will save its contents to the file add.cpp.\n",
        "\n",
        "The following cell will compile and run this C++ program."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNpH54M_RbAU",
        "outputId": "59edb19f-eca0-4309-dc58-be3d298b5037"
      },
      "outputs": [
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "g++ add.cpp -o add"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6V2tGPYRi3l"
      },
      "source": [
        "Then run it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmA4ACe5RuiU",
        "outputId": "71eaa1c6-8fe1-4a58-a59a-a8f00ed58b2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max error: 0\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "./add"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IAWYlniR153"
      },
      "source": [
        "As expected, it prints that there was no error in the summation and then exits. Now I want to get this computation running (in parallel) on the many cores of a GPU. It’s actually pretty easy to take the first steps.\n",
        "\n",
        "First, I just have to turn our `add` function into a function that the GPU can run, called a *kernel* in CUDA. To do this, all I have to do is add the specifier `__global__` to the function, which tells the CUDA C++ compiler that this is a function that runs on the GPU and can be called from CPU code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heY-lpzjSHfB"
      },
      "source": [
        "```cpp\n",
        "// CUDA Kernel function to add the elements of two arrays on the GPU\n",
        "__global__\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "  for (int i = 0; i < n; i++)\n",
        "      y[i] = x[i] + y[i];\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kozMbHdpSKNu"
      },
      "source": [
        "These `__global__` functions are known as *kernels*, and code that runs on the GPU is often called *device code*, while code that runs on the CPU is *host code*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhnBGGU-SWiN"
      },
      "source": [
        "## Memory Allocation in CUDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvIDRBk2SbqA"
      },
      "source": [
        "To compute on the GPU, I need to allocate memory accessible by the GPU. [Unified Memory](https://developer.nvidia.com/blog/unified-memory-in-cuda-6/) in CUDA makes this easy by providing a single memory space accessible by all GPUs and CPUs in your system. To allocate data in unified memory, call `cudaMallocManaged()`, which returns a pointer that you can access from host (CPU) code or device (GPU) code. To free the data, just pass the pointer to `cudaFree()`.\n",
        "\n",
        "I just need to replace the calls to `new` in the code above with calls to `cudaMallocManaged()`, and replace calls to `delete []` with calls to `cudaFree`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxCut_urS46H"
      },
      "source": [
        "```cpp\n",
        "  // Allocate Unified Memory -- accessible from CPU or GPU\n",
        "  float *x, *y;\n",
        "  cudaMallocManaged(&x, N*sizeof(float));\n",
        "  cudaMallocManaged(&y, N*sizeof(float));\n",
        "\n",
        "  ...\n",
        "\n",
        "  // Free memory\n",
        "  cudaFree(x);\n",
        "  cudaFree(y);\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oEf2B-1S-1V"
      },
      "source": [
        "Finally, I need to *launch* the `add()` kernel, which invokes it on the GPU. CUDA kernel launches are specified using the triple angle bracket syntax `<<< >>>`. I just have to add it to the call to `add` before the parameter list."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqTJlvWLS7iW"
      },
      "source": [
        "```cpp\n",
        "add<<<1, 1>>>(N, x, y);\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGf0ZiTOTTHU"
      },
      "source": [
        "Easy! I’ll get into the details of what goes inside the angle brackets soon; for now all you need to know is that this line launches one GPU thread to run `add()`.\n",
        "\n",
        "Just one more thing: I need the CPU to wait until the kernel is done before it accesses the results (because CUDA kernel launches don’t block the calling CPU thread). To do this I just call `cudaDeviceSynchronize()` before doing the final error checking on the CPU.\n",
        "\n",
        "Here’s the complete code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8bYDM7kYT7S",
        "outputId": "8d315620-1b9e-4df4-8b20-0a22cb38d0aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting add.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile add.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "\n",
        "#define CHECK_CUDA_ERROR(call) \\\n",
        "do { \\\n",
        "  cudaError_t err = call; \\\n",
        "  if (err != cudaSuccess) { \\\n",
        "    std::cerr << \"CUDA error in \" << __FILE__ << \":\" << __LINE__ << \": \" \\\n",
        "              << cudaGetErrorString(err) << std::endl; \\\n",
        "    exit(1); \\\n",
        "  } \\\n",
        "} while(0)\n",
        "\n",
        "// Kernel function to add the elements of two arrays\n",
        "__global__\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "  for (int i = 0; i < n; i++)\n",
        "    y[i] = x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  int N = 1<<20;\n",
        "  float *x, *y;\n",
        "\n",
        "  // Allocate Unified Memory – accessible from CPU or GPU\n",
        "  CHECK_CUDA_ERROR(cudaMallocManaged(&x, N*sizeof(float)));\n",
        "  CHECK_CUDA_ERROR(cudaMallocManaged(&y, N*sizeof(float)));\n",
        "\n",
        "  // initialize x and y arrays on the host\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        "\n",
        "  // Run kernel on 1M elements on the GPU\n",
        "  add<<<1, 1>>>(N, x, y);\n",
        "  CHECK_CUDA_ERROR(cudaGetLastError());\n",
        "\n",
        "  // Wait for GPU to finish before accessing on host\n",
        "  CHECK_CUDA_ERROR(cudaDeviceSynchronize());\n",
        "\n",
        "  // Check for errors (all values should be 3.0f)\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
        "  std::cout << \"Max error: \" << maxError << std::endl;\n",
        "\n",
        "  // Free memory\n",
        "  cudaFree(x);\n",
        "  cudaFree(y);\n",
        "\n",
        "  return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check GPU and CUDA Version Compatibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat Dec  6 04:10:44 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "# Check NVIDIA GPU and driver info\n",
        "nvidia-smi\n",
        "\n",
        "# Check NVCC version\n",
        "nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjLGGp0oYeEc",
        "outputId": "66a7a206-9258-46d4-d87a-aa435465a0b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max error: 0\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "# Compile with architecture flags compatible with Colab GPUs\n",
        "# Use compute capability 7.5 for T4 or 8.0 for A100\n",
        "nvcc -arch=sm_75 add.cu -o add_cuda\n",
        "./add_cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ATssEzEYqGx"
      },
      "source": [
        "This is only a first step, because as written, this kernel is only correct for a single thread, since every thread that runs it will perform the add on the whole array. Moreover, there is a [race condition](https://en.wikipedia.org/wiki/Race_condition) since multiple parallel threads would both read and write the same locations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kKpDoZ-YzJ8"
      },
      "source": [
        "## Profile it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-BC-CWVZglt"
      },
      "source": [
        "I think the simplest way to find out how long the kernel takes to run is to run it with `nvprof`, the command line GPU profiler that comes with the CUDA Toolkit. Just type `nvprof ./add_cuda` on the command line:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtfQLWwYZpfV",
        "outputId": "1254d3df-afc0-44ef-e1d9-9c8fece4da9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==5574== NVPROF is profiling process 5574, command: ./add_cuda\n",
            "Max error: 0\n",
            "==5574== Profiling application: ./add_cuda\n",
            "==5574== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:  100.00%  108.92ms         1  108.92ms  108.92ms  108.92ms  add(int, float*, float*)\n",
            "      API calls:   52.76%  108.93ms         1  108.93ms  108.93ms  108.93ms  cudaDeviceSynchronize\n",
            "                   46.85%  96.731ms         2  48.365ms  42.156us  96.688ms  cudaMallocManaged\n",
            "                    0.23%  481.93us         2  240.96us  225.86us  256.07us  cudaFree\n",
            "                    0.08%  161.19us         1  161.19us  161.19us  161.19us  cudaLaunchKernel\n",
            "                    0.07%  151.75us       114  1.3310us     109ns  66.339us  cuDeviceGetAttribute\n",
            "                    0.01%  14.157us         1  14.157us  14.157us  14.157us  cuDeviceGetName\n",
            "                    0.00%  5.7520us         1  5.7520us  5.7520us  5.7520us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.7980us         3     599ns     167ns  1.3190us  cuDeviceGetCount\n",
            "                    0.00%  1.1300us         2     565ns     235ns     895ns  cuDeviceGet\n",
            "                    0.00%     653ns         1     653ns     653ns     653ns  cuDeviceTotalMem\n",
            "                    0.00%     432ns         1     432ns     432ns     432ns  cuModuleGetLoadingMode\n",
            "                    0.00%     412ns         1     412ns     412ns     412ns  cudaGetLastError\n",
            "                    0.00%     283ns         1     283ns     283ns     283ns  cuDeviceGetUuid\n",
            "\n",
            "==5574== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "      48  170.67KB  4.0000KB  0.9961MB  8.000000MB  806.1880us  Host To Device\n",
            "      24  170.67KB  4.0000KB  0.9961MB  4.000000MB  361.7190us  Device To Host\n",
            "      12         -         -         -           -  2.656223ms  Gpu page fault groups\n",
            "Total CPU Page faults: 36\n",
            "==5574== Profiling application: ./add_cuda\n",
            "==5574== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:  100.00%  108.92ms         1  108.92ms  108.92ms  108.92ms  add(int, float*, float*)\n",
            "      API calls:   52.76%  108.93ms         1  108.93ms  108.93ms  108.93ms  cudaDeviceSynchronize\n",
            "                   46.85%  96.731ms         2  48.365ms  42.156us  96.688ms  cudaMallocManaged\n",
            "                    0.23%  481.93us         2  240.96us  225.86us  256.07us  cudaFree\n",
            "                    0.08%  161.19us         1  161.19us  161.19us  161.19us  cudaLaunchKernel\n",
            "                    0.07%  151.75us       114  1.3310us     109ns  66.339us  cuDeviceGetAttribute\n",
            "                    0.01%  14.157us         1  14.157us  14.157us  14.157us  cuDeviceGetName\n",
            "                    0.00%  5.7520us         1  5.7520us  5.7520us  5.7520us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.7980us         3     599ns     167ns  1.3190us  cuDeviceGetCount\n",
            "                    0.00%  1.1300us         2     565ns     235ns     895ns  cuDeviceGet\n",
            "                    0.00%     653ns         1     653ns     653ns     653ns  cuDeviceTotalMem\n",
            "                    0.00%     432ns         1     432ns     432ns     432ns  cuModuleGetLoadingMode\n",
            "                    0.00%     412ns         1     412ns     412ns     412ns  cudaGetLastError\n",
            "                    0.00%     283ns         1     283ns     283ns     283ns  cuDeviceGetUuid\n",
            "\n",
            "==5574== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "      48  170.67KB  4.0000KB  0.9961MB  8.000000MB  806.1880us  Host To Device\n",
            "      24  170.67KB  4.0000KB  0.9961MB  4.000000MB  361.7190us  Device To Host\n",
            "      12         -         -         -           -  2.656223ms  Gpu page fault groups\n",
            "Total CPU Page faults: 36\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "nvprof ./add_cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9Dn4ZV-Z_UJ"
      },
      "source": [
        "The above will show the single call to `add`. Your timing may vary depending on the GPU allocated to you by Colab. To see the current GPU allocated to you run the following cell and look in the `Name` column where you might see, for example `Tesla T4`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrYmwVZfaPqz",
        "outputId": "842b124d-7e43-4312-b022-5b7d2b2089a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat Dec  6 04:10:47 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0             25W /   70W |       0MiB /  15360MiB |     58%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MWYteAVadCs"
      },
      "source": [
        "Let's make it faster with parallelism."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaiMC73Falvb"
      },
      "source": [
        "## Picking up the Threads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDFuBr_2apuJ"
      },
      "source": [
        "Now that you’ve run a kernel with one thread that does some computation, how do you make it parallel? The key is in CUDA’s `<<<1, 1>>>` syntax. This is called the execution configuration, and it tells the CUDA runtime how many parallel threads to use for the launch on the GPU. There are two parameters here, but let’s start by changing the second one: the number of threads in a thread block. CUDA GPUs run kernels using blocks of threads that are a multiple of 32 in size, so 256 threads is a reasonable size to choose."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2Pmyj0KavgB"
      },
      "source": [
        "```cpp\n",
        "add<<<1, 256>>>(N, x, y);\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAYpH9Ctay5G"
      },
      "source": [
        "If I run the code with only this change, it will do the computation once per thread, rather than spreading the computation across the parallel threads. To do it properly, I need to modify the kernel. CUDA C++ provides keywords that let kernels get the indices of the running threads. Specifically, `threadIdx.x` contains the index of the current thread within its block, and `blockDim.x` contains the number of threads in the block. I’ll just modify the loop to stride through the array with parallel threads."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSiqhFK_a6N3"
      },
      "source": [
        "```cpp\n",
        "__global__\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "  int index = threadIdx.x;\n",
        "  int stride = blockDim.x;\n",
        "  for (int i = index; i < n; i += stride)\n",
        "      y[i] = x[i] + y[i];\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7mYcBzOa9zR"
      },
      "source": [
        "The `add` function hasn’t changed that much. In fact, setting `index` to 0 and `stride` to 1 makes it semantically identical to the first version.\n",
        "\n",
        "Here we save the file as add_block.cu and compile and run it in `nvprof` again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goCKY9QNbPZ-",
        "outputId": "7d76a002-52ed-4fe6-aeb5-b536c67c416b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting add_block.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile add_block.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "\n",
        "#define CHECK_CUDA_ERROR(call) \\\n",
        "do { \\\n",
        "  cudaError_t err = call; \\\n",
        "  if (err != cudaSuccess) { \\\n",
        "    std::cerr << \"CUDA error in \" << __FILE__ << \":\" << __LINE__ << \": \" \\\n",
        "              << cudaGetErrorString(err) << std::endl; \\\n",
        "    exit(1); \\\n",
        "  } \\\n",
        "} while(0)\n",
        "\n",
        "__global__\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "  int index = threadIdx.x;\n",
        "  int stride = blockDim.x;\n",
        "  for (int i = index; i < n; i += stride)\n",
        "      y[i] = x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  int N = 1<<20;\n",
        "  float *x, *y;\n",
        "\n",
        "  // Allocate Unified Memory – accessible from CPU or GPU\n",
        "  CHECK_CUDA_ERROR(cudaMallocManaged(&x, N*sizeof(float)));\n",
        "  CHECK_CUDA_ERROR(cudaMallocManaged(&y, N*sizeof(float)));\n",
        "\n",
        "  // initialize x and y arrays on the host\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        "\n",
        "  // Run kernel on 1M elements on the GPU\n",
        "  add<<<1, 256>>>(N, x, y);\n",
        "  CHECK_CUDA_ERROR(cudaGetLastError());\n",
        "\n",
        "  // Wait for GPU to finish before accessing on host\n",
        "  CHECK_CUDA_ERROR(cudaDeviceSynchronize());\n",
        "\n",
        "  // Check for errors (all values should be 3.0f)\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
        "  std::cout << \"Max error: \" << maxError << std::endl;\n",
        "\n",
        "  // Free memory\n",
        "  cudaFree(x);\n",
        "  cudaFree(y);\n",
        "\n",
        "  return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9cmfbcVbYgD",
        "outputId": "2fbadf45-de3a-455a-f7bf-a8dbfc071cf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==5631== NVPROF is profiling process 5631, command: ./add_block\n",
            "Max error: 0\n",
            "==5631== Profiling application: ./add_block\n",
            "==5631== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:  100.00%  3.5265ms         1  3.5265ms  3.5265ms  3.5265ms  add(int, float*, float*)\n",
            "      API calls:   96.02%  104.75ms         2  52.374ms  60.512us  104.69ms  cudaMallocManaged\n",
            "                    3.24%  3.5330ms         1  3.5330ms  3.5330ms  3.5330ms  cudaDeviceSynchronize\n",
            "                    0.43%  472.73us         2  236.37us  225.91us  246.82us  cudaFree\n",
            "                    0.16%  174.51us         1  174.51us  174.51us  174.51us  cudaLaunchKernel\n",
            "                    0.13%  137.10us       114  1.2020us     105ns  56.892us  cuDeviceGetAttribute\n",
            "                    0.01%  12.162us         1  12.162us  12.162us  12.162us  cuDeviceGetName\n",
            "                    0.00%  4.6430us         1  4.6430us  4.6430us  4.6430us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.7660us         3     588ns     121ns  1.4510us  cuDeviceGetCount\n",
            "                    0.00%     725ns         2     362ns     112ns     613ns  cuDeviceGet\n",
            "                    0.00%     508ns         1     508ns     508ns     508ns  cuDeviceTotalMem\n",
            "                    0.00%     473ns         1     473ns     473ns     473ns  cudaGetLastError\n",
            "                    0.00%     383ns         1     383ns     383ns     383ns  cuModuleGetLoadingMode\n",
            "                    0.00%     287ns         1     287ns     287ns     287ns  cuDeviceGetUuid\n",
            "\n",
            "==5631== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "      48  170.67KB  4.0000KB  0.9961MB  8.000000MB  805.5770us  Host To Device\n",
            "      24  170.67KB  4.0000KB  0.9961MB  4.000000MB  361.1740us  Device To Host\n",
            "      12         -         -         -           -  1.914926ms  Gpu page fault groups\n",
            "Total CPU Page faults: 36\n",
            "Max error: 0\n",
            "==5631== Profiling application: ./add_block\n",
            "==5631== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:  100.00%  3.5265ms         1  3.5265ms  3.5265ms  3.5265ms  add(int, float*, float*)\n",
            "      API calls:   96.02%  104.75ms         2  52.374ms  60.512us  104.69ms  cudaMallocManaged\n",
            "                    3.24%  3.5330ms         1  3.5330ms  3.5330ms  3.5330ms  cudaDeviceSynchronize\n",
            "                    0.43%  472.73us         2  236.37us  225.91us  246.82us  cudaFree\n",
            "                    0.16%  174.51us         1  174.51us  174.51us  174.51us  cudaLaunchKernel\n",
            "                    0.13%  137.10us       114  1.2020us     105ns  56.892us  cuDeviceGetAttribute\n",
            "                    0.01%  12.162us         1  12.162us  12.162us  12.162us  cuDeviceGetName\n",
            "                    0.00%  4.6430us         1  4.6430us  4.6430us  4.6430us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.7660us         3     588ns     121ns  1.4510us  cuDeviceGetCount\n",
            "                    0.00%     725ns         2     362ns     112ns     613ns  cuDeviceGet\n",
            "                    0.00%     508ns         1     508ns     508ns     508ns  cuDeviceTotalMem\n",
            "                    0.00%     473ns         1     473ns     473ns     473ns  cudaGetLastError\n",
            "                    0.00%     383ns         1     383ns     383ns     383ns  cuModuleGetLoadingMode\n",
            "                    0.00%     287ns         1     287ns     287ns     287ns  cuDeviceGetUuid\n",
            "\n",
            "==5631== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "      48  170.67KB  4.0000KB  0.9961MB  8.000000MB  805.5770us  Host To Device\n",
            "      24  170.67KB  4.0000KB  0.9961MB  4.000000MB  361.1740us  Device To Host\n",
            "      12         -         -         -           -  1.914926ms  Gpu page fault groups\n",
            "Total CPU Page faults: 36\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "nvcc -arch=sm_75 add_block.cu -o add_block\n",
        "nvprof ./add_block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo5KaV3Nba7g"
      },
      "source": [
        "That’s a big speedup (compare the time for the `add` kernel by looking at the `GPU activities` field), but not surprising since I went from 1 thread to 256 threads. Let’s keep going to get even more performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtgQWOyMcPfn"
      },
      "source": [
        "## Out of the Blocks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAoFGwmbcRbN"
      },
      "source": [
        "CUDA GPUs have many parallel processors grouped into Streaming Multiprocessors, or SMs. Each SM can run multiple concurrent thread blocks. As an example, a Tesla P100 GPU based on the [Pascal GPU Architecture](https://developer.nvidia.com/blog/inside-pascal/) has 56 SMs, each capable of supporting up to 2048 active threads. To take full advantage of all these threads, I should launch the kernel with multiple thread blocks.\n",
        "\n",
        "By now you may have guessed that the first parameter of the execution configuration specifies the number of thread blocks. Together, the blocks of parallel threads make up what is known as the *grid*. Since I have `N` elements to process, and 256 threads per block, I just need to calculate the number of blocks to get at least `N` threads. I simply divide `N` by the block size (being careful to round up in case `N` is not a multiple of `blockSize`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnI2II2ockgC"
      },
      "source": [
        "```cpp\n",
        "int blockSize = 256;\n",
        "int numBlocks = (N + blockSize - 1) / blockSize;\n",
        "add<<<numBlocks, blockSize>>>(N, x, y);\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayq2MJZLctY0"
      },
      "source": [
        "<img src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2017/01/cuda_indexing.png\" width=\"800\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZduP7RWc3Je"
      },
      "source": [
        "I also need to update the kernel code to take into account the entire grid of thread blocks. CUDA provides `gridDim.x`, which contains the number of blocks in the grid, and `blockIdx.x`, which contains the index of the current thread block in the grid. Figure 1 illustrates the the approach to indexing into an array (one-dimensional) in CUDA using `blockDim.x`, `gridDim.x`, and `threadIdx.x`. The idea is that each thread gets its index by computing the offset to the beginning of its block (the block index times the block size: `blockIdx.x * blockDim.x`) and adding the thread’s index within the block (`threadIdx.x`). The code `blockIdx.x * blockDim.x + threadIdx.x` is idiomatic CUDA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cI2WLEAeG5y"
      },
      "source": [
        "```cpp\n",
        "__global__\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "  for (int i = index; i < n; i += stride)\n",
        "    y[i] = x[i] + y[i];\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83hC-rCLdPHC"
      },
      "source": [
        "The updated kernel also sets stride to the total number of threads in the grid (`blockDim.x * gridDim.x`). This type of loop in a CUDA kernel is often called a [*grid-stride*](https://developer.nvidia.com/blog/cuda-pro-tip-write-flexible-kernels-grid-stride-loops/) loop.\n",
        "\n",
        "Save the file as `add_grid.cu` and compile and run it in `nvprof` again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7w-DHBRdhUC",
        "outputId": "c5acabdd-7284-48bd-e56d-cf8216b9f8e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting add_grid.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile add_grid.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "\n",
        "#define CHECK_CUDA_ERROR(call) \\\n",
        "do { \\\n",
        "  cudaError_t err = call; \\\n",
        "  if (err != cudaSuccess) { \\\n",
        "    std::cerr << \"CUDA error in \" << __FILE__ << \":\" << __LINE__ << \": \" \\\n",
        "              << cudaGetErrorString(err) << std::endl; \\\n",
        "    exit(1); \\\n",
        "  } \\\n",
        "} while(0)\n",
        "\n",
        "__global__\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "  for (int i = index; i < n; i += stride)\n",
        "    y[i] = x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  int N = 1<<20;\n",
        "  float *x, *y;\n",
        "\n",
        "  // Allocate Unified Memory – accessible from CPU or GPU\n",
        "  CHECK_CUDA_ERROR(cudaMallocManaged(&x, N*sizeof(float)));\n",
        "  CHECK_CUDA_ERROR(cudaMallocManaged(&y, N*sizeof(float)));\n",
        "\n",
        "  // initialize x and y arrays on the host\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        "\n",
        "  // Run kernel on 1M elements on the GPU\n",
        "  int blockSize = 256;\n",
        "  int numBlocks = (N + blockSize - 1) / blockSize;\n",
        "  add<<<numBlocks, blockSize>>>(N, x, y);\n",
        "  CHECK_CUDA_ERROR(cudaGetLastError());\n",
        "\n",
        "  // Wait for GPU to finish before accessing on host\n",
        "  CHECK_CUDA_ERROR(cudaDeviceSynchronize());\n",
        "\n",
        "  // Check for errors (all values should be 3.0f)\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
        "  std::cout << \"Max error: \" << maxError << std::endl;\n",
        "\n",
        "  // Free memory\n",
        "  cudaFree(x);\n",
        "  cudaFree(y);\n",
        "\n",
        "  return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhcrktW9dw34",
        "outputId": "380ce442-eb45-479e-a1f1-4c6004e09f82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==5683== NVPROF is profiling process 5683, command: ./add_grid\n",
            "Max error: 0\n",
            "==5683== Profiling application: ./add_grid\n",
            "==5683== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:  100.00%  2.3388ms         1  2.3388ms  2.3388ms  2.3388ms  add(int, float*, float*)\n",
            "      API calls:   96.91%  101.08ms         2  50.540ms  68.583us  101.01ms  cudaMallocManaged\n",
            "                    2.26%  2.3596ms         1  2.3596ms  2.3596ms  2.3596ms  cudaDeviceSynchronize\n",
            "                    0.51%  529.82us         2  264.91us  227.95us  301.87us  cudaFree\n",
            "                    0.17%  176.12us         1  176.12us  176.12us  176.12us  cudaLaunchKernel\n",
            "                    0.13%  136.40us       114  1.1960us     103ns  55.690us  cuDeviceGetAttribute\n",
            "                    0.01%  13.271us         1  13.271us  13.271us  13.271us  cuDeviceGetName\n",
            "                    0.01%  5.2830us         1  5.2830us  5.2830us  5.2830us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.7070us         3     569ns     131ns  1.3370us  cuDeviceGetCount\n",
            "                    0.00%     900ns         2     450ns     148ns     752ns  cuDeviceGet\n",
            "                    0.00%     594ns         1     594ns     594ns     594ns  cuDeviceTotalMem\n",
            "                    0.00%     552ns         1     552ns     552ns     552ns  cuModuleGetLoadingMode\n",
            "                    0.00%     386ns         1     386ns     386ns     386ns  cudaGetLastError\n",
            "                    0.00%     322ns         1     322ns     322ns     322ns  cuDeviceGetUuid\n",
            "\n",
            "==5683== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "      86  95.255KB  4.0000KB  988.00KB  8.000000MB  905.3870us  Host To Device\n",
            "      24  170.67KB  4.0000KB  0.9961MB  4.000000MB  360.9200us  Device To Host\n",
            "      11         -         -         -           -  2.280488ms  Gpu page fault groups\n",
            "Total CPU Page faults: 36\n",
            "Max error: 0\n",
            "==5683== Profiling application: ./add_grid\n",
            "==5683== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:  100.00%  2.3388ms         1  2.3388ms  2.3388ms  2.3388ms  add(int, float*, float*)\n",
            "      API calls:   96.91%  101.08ms         2  50.540ms  68.583us  101.01ms  cudaMallocManaged\n",
            "                    2.26%  2.3596ms         1  2.3596ms  2.3596ms  2.3596ms  cudaDeviceSynchronize\n",
            "                    0.51%  529.82us         2  264.91us  227.95us  301.87us  cudaFree\n",
            "                    0.17%  176.12us         1  176.12us  176.12us  176.12us  cudaLaunchKernel\n",
            "                    0.13%  136.40us       114  1.1960us     103ns  55.690us  cuDeviceGetAttribute\n",
            "                    0.01%  13.271us         1  13.271us  13.271us  13.271us  cuDeviceGetName\n",
            "                    0.01%  5.2830us         1  5.2830us  5.2830us  5.2830us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.7070us         3     569ns     131ns  1.3370us  cuDeviceGetCount\n",
            "                    0.00%     900ns         2     450ns     148ns     752ns  cuDeviceGet\n",
            "                    0.00%     594ns         1     594ns     594ns     594ns  cuDeviceTotalMem\n",
            "                    0.00%     552ns         1     552ns     552ns     552ns  cuModuleGetLoadingMode\n",
            "                    0.00%     386ns         1     386ns     386ns     386ns  cudaGetLastError\n",
            "                    0.00%     322ns         1     322ns     322ns     322ns  cuDeviceGetUuid\n",
            "\n",
            "==5683== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "      86  95.255KB  4.0000KB  988.00KB  8.000000MB  905.3870us  Host To Device\n",
            "      24  170.67KB  4.0000KB  0.9961MB  4.000000MB  360.9200us  Device To Host\n",
            "      11         -         -         -           -  2.280488ms  Gpu page fault groups\n",
            "Total CPU Page faults: 36\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "nvcc -arch=sm_75 add_grid.cu -o add_grid\n",
        "nvprof ./add_grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7Tz-xo3d1oX"
      },
      "source": [
        "That's another big speedup from running multiple blocks! (Note your results may vary from the blog post due to whatever GPU you've been allocated by Colab. If you notice your speedups for the final example are not as drastic as those in the blog post, check out #4 in the *Exercises* section below.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ja5CiQZpicHC"
      },
      "source": [
        "## Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEijwk25id3t"
      },
      "source": [
        "To keep you going, here are a few things to try on your own.\n",
        "\n",
        "1. Browse the [CUDA Toolkit documentation](https://docs.nvidia.com/cuda/index.html). If you haven’t installed CUDA yet, check out the [Quick Start Guide](https://docs.nvidia.com/cuda/cuda-quick-start-guide/index.html) and the installation guides. Then browse the [Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html) and the [Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html). There are also tuning guides for various architectures.\n",
        "2. Experiment with `printf()` inside the kernel. Try printing out the values of `threadIdx.x` and `blockIdx.x` for some or all of the threads. Do they print in sequential order? Why or why not?\n",
        "3. Print the value of `threadIdx.y` or `threadIdx.z` (or `blockIdx.y`) in the kernel. (Likewise for `blockDim` and `gridDim`). Why do these exist? How do you get them to take on values other than 0 (1 for the dims)?\n",
        "4. If you have access to a [Pascal-based GPU](https://developer.nvidia.com/blog/inside-pascal/), try running `add_grid.cu` on it. Is performance better or worse than the K80 results? Why? (Hint: read about [Pascal’s Page Migration Engine and the CUDA 8 Unified Memory API](https://developer.nvidia.com/blog/beyond-gpu-memory-limits-unified-memory-pascal/).) For a detailed answer to this question, see the post [Unified Memory for CUDA Beginners](https://developer.nvidia.com/blog/unified-memory-cuda-beginners/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise Solutions\n",
        "\n",
        "Let's work through the exercises to deepen our understanding of CUDA programming."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2: Experiment with `printf()` inside the kernel\n",
        "\n",
        "Let's create a version of our kernel that prints thread and block indices to understand the execution order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting printf_exercise.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile printf_exercise.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "\n",
        "#define CHECK_CUDA_ERROR(call) \\\n",
        "do { \\\n",
        "  cudaError_t err = call; \\\n",
        "  if (err != cudaSuccess) { \\\n",
        "    std::cerr << \"CUDA error in \" << __FILE__ << \":\" << __LINE__ << \": \" \\\n",
        "              << cudaGetErrorString(err) << std::endl; \\\n",
        "    exit(1); \\\n",
        "  } \\\n",
        "} while(0)\n",
        "\n",
        "// Kernel function with printf to show thread execution order\n",
        "__global__\n",
        "void add_with_printf(int n, float *x, float *y)\n",
        "{\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "\n",
        "  // Print only for the first few threads to avoid overwhelming output\n",
        "  if (index < 10) {\n",
        "    printf(\"Thread %d: blockIdx.x=%d, threadIdx.x=%d, computing index=%d\\n\",\n",
        "           index, blockIdx.x, threadIdx.x, index);\n",
        "  }\n",
        "\n",
        "  for (int i = index; i < n; i += stride)\n",
        "    y[i] = x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  int N = 1<<20;\n",
        "  float *x, *y;\n",
        "\n",
        "  // Allocate Unified Memory\n",
        "  CHECK_CUDA_ERROR(cudaMallocManaged(&x, N*sizeof(float)));\n",
        "  CHECK_CUDA_ERROR(cudaMallocManaged(&y, N*sizeof(float)));\n",
        "\n",
        "  // Initialize arrays\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        "\n",
        "  // Run kernel with multiple blocks\n",
        "  int blockSize = 256;\n",
        "  int numBlocks = (N + blockSize - 1) / blockSize;\n",
        "  add_with_printf<<<numBlocks, blockSize>>>(N, x, y);\n",
        "  CHECK_CUDA_ERROR(cudaGetLastError());\n",
        "\n",
        "  // Wait for GPU to finish\n",
        "  CHECK_CUDA_ERROR(cudaDeviceSynchronize());\n",
        "\n",
        "  // Check for errors\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
        "  std::cout << \"Max error: \" << maxError << std::endl;\n",
        "\n",
        "  // Free memory\n",
        "  cudaFree(x);\n",
        "  cudaFree(y);\n",
        "\n",
        "  return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thread 0: blockIdx.x=0, threadIdx.x=0, computing index=0\n",
            "Thread 1: blockIdx.x=0, threadIdx.x=1, computing index=1\n",
            "Thread 2: blockIdx.x=0, threadIdx.x=2, computing index=2\n",
            "Thread 3: blockIdx.x=0, threadIdx.x=3, computing index=3\n",
            "Thread 4: blockIdx.x=0, threadIdx.x=4, computing index=4\n",
            "Thread 5: blockIdx.x=0, threadIdx.x=5, computing index=5\n",
            "Thread 6: blockIdx.x=0, threadIdx.x=6, computing index=6\n",
            "Thread 7: blockIdx.x=0, threadIdx.x=7, computing index=7\n",
            "Thread 8: blockIdx.x=0, threadIdx.x=8, computing index=8\n",
            "Thread 9: blockIdx.x=0, threadIdx.x=9, computing index=9\n",
            "Max error: 0\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "nvcc -arch=sm_75 printf_exercise.cu -o printf_exercise\n",
        "./printf_exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Observation:** The threads do NOT print in sequential order! This is because:\n",
        "\n",
        "1. **Parallel Execution**: Threads execute in parallel on different cores/SMs\n",
        "2. **No Synchronization**: There's no guarantee about the order of execution between different blocks or even threads within a block\n",
        "3. **Warp Scheduling**: The GPU scheduler organizes threads into warps (groups of 32) and executes them when resources are available\n",
        "4. **Hardware Dependent**: The order can vary based on SM availability, workload, and GPU architecture\n",
        "\n",
        "This demonstrates that CUDA threads execute asynchronously and independently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3: Exploring Multi-Dimensional Thread Indexing\n",
        "\n",
        "CUDA supports 3D grids and 3D blocks. Let's explore `threadIdx.y`, `threadIdx.z`, `blockIdx.y`, `blockDim.y`, and `gridDim.y`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting multidim_exercise.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile multidim_exercise.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "\n",
        "#define CHECK_CUDA_ERROR(call) \\\n",
        "do { \\\n",
        "  cudaError_t err = call; \\\n",
        "  if (err != cudaSuccess) { \\\n",
        "    std::cerr << \"CUDA error in \" << __FILE__ << \":\" << __LINE__ << \": \" \\\n",
        "              << cudaGetErrorString(err) << std::endl; \\\n",
        "    exit(1); \\\n",
        "  } \\\n",
        "} while(0)\n",
        "\n",
        "// 2D kernel using multi-dimensional grids and blocks\n",
        "__global__\n",
        "void add_2d(int n, float *x, float *y)\n",
        "{\n",
        "  // Calculate 1D index from 2D grid structure\n",
        "  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int idy = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "  // Convert 2D coordinates to 1D array index\n",
        "  int gridWidth = gridDim.x * blockDim.x;\n",
        "  int index = idy * gridWidth + idx;\n",
        "\n",
        "  // Grid-stride loop to handle arrays larger than grid\n",
        "  int stride_x = blockDim.x * gridDim.x;\n",
        "  int stride_y = blockDim.y * gridDim.y;\n",
        "\n",
        "  // Print info for a few threads\n",
        "  if (idx < 2 && idy < 2) {\n",
        "    printf(\"Thread (%d,%d): blockIdx=(%d,%d), threadIdx=(%d,%d), index=%d\\n\",\n",
        "           idx, idy, blockIdx.x, blockIdx.y, threadIdx.x, threadIdx.y, index);\n",
        "  }\n",
        "\n",
        "  // Process elements with 2D stride\n",
        "  for (int j = idy; j < n; j += stride_y) {\n",
        "    for (int i = idx; i < n; i += stride_x) {\n",
        "      int pos = j * n + i;\n",
        "      if (pos < n * n)\n",
        "        y[pos] = x[pos] + y[pos];\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  int N = 1<<10; // Use smaller N for 2D (1024x1024 elements)\n",
        "  float *x, *y;\n",
        "\n",
        "  // Allocate Unified Memory\n",
        "  CHECK_CUDA_ERROR(cudaMallocManaged(&x, N*N*sizeof(float)));\n",
        "  CHECK_CUDA_ERROR(cudaMallocManaged(&y, N*N*sizeof(float)));\n",
        "\n",
        "  // Initialize arrays\n",
        "  for (int i = 0; i < N*N; i++) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        "\n",
        "  // Launch with 2D grid and 2D blocks\n",
        "  dim3 threadsPerBlock(16, 16);\n",
        "  dim3 numBlocks((N + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
        "                 (N + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
        "\n",
        "  std::cout << \"Launching kernel with grid(\" << numBlocks.x << \",\" << numBlocks.y\n",
        "            << \") and blocks(\" << threadsPerBlock.x << \",\" << threadsPerBlock.y << \")\\n\";\n",
        "\n",
        "  add_2d<<<numBlocks, threadsPerBlock>>>(N, x, y);\n",
        "  CHECK_CUDA_ERROR(cudaGetLastError());\n",
        "\n",
        "  // Wait for GPU to finish\n",
        "  CHECK_CUDA_ERROR(cudaDeviceSynchronize());\n",
        "\n",
        "  // Check for errors\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N*N; i++)\n",
        "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
        "  std::cout << \"Max error: \" << maxError << std::endl;\n",
        "\n",
        "  // Free memory\n",
        "  cudaFree(x);\n",
        "  cudaFree(y);\n",
        "\n",
        "  return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Launching kernel with grid(64,64) and blocks(16,16)\n",
            "Thread (0,0): blockIdx=(0,0), threadIdx=(0,0), index=0\n",
            "Thread (1,0): blockIdx=(0,0), threadIdx=(1,0), index=1\n",
            "Thread (0,1): blockIdx=(0,0), threadIdx=(0,1), index=1024\n",
            "Thread (1,1): blockIdx=(0,0), threadIdx=(1,1), index=1025\n",
            "Max error: 0\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "nvcc -arch=sm_75 multidim_exercise.cu -o multidim_exercise\n",
        "./multidim_exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why do multi-dimensional indices exist?**\n",
        "\n",
        "1. **Natural Mapping**: Many problems are naturally 2D (images, matrices) or 3D (volumes, simulations)\n",
        "2. **Code Clarity**: Using 2D/3D indexing makes code more readable for matrix/image operations\n",
        "3. **Memory Access Patterns**: Helps optimize memory coalescing by matching problem structure\n",
        "\n",
        "**How to use them:**\n",
        "\n",
        "- Use `dim3` type to specify grid and block dimensions:\n",
        "  ```cpp\n",
        "  dim3 blockSize(16, 16, 1);    // 16x16x1 = 256 threads per block\n",
        "  dim3 gridSize(32, 32, 1);     // 32x32x1 = 1024 blocks\n",
        "  kernel<<<gridSize, blockSize>>>(...)\n",
        "  ```\n",
        "\n",
        "**Default values:** If not specified, dimensions default to 1. In our 1D examples:\n",
        "- `threadIdx.y = 0`, `threadIdx.z = 0`\n",
        "- `blockDim.y = 1`, `blockDim.z = 1`\n",
        "- Similar for `blockIdx` and `gridDim`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 4: GPU Architecture and Performance Analysis\n",
        "\n",
        "Let's check what GPU is available and analyze the performance characteristics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "name, compute_cap, memory.total [MiB]\n",
            "Tesla T4, 7.5, 15360 MiB\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "# Check GPU information\n",
        "nvidia-smi --query-gpu=name,compute_cap,memory.total --format=csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting gpu_info.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile gpu_info.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "int main() {\n",
        "    int deviceCount;\n",
        "    cudaGetDeviceCount(&deviceCount);\n",
        "\n",
        "    for (int dev = 0; dev < deviceCount; dev++) {\n",
        "        cudaDeviceProp prop;\n",
        "        cudaGetDeviceProperties(&prop, dev);\n",
        "\n",
        "        std::cout << \"\\n=== GPU Device \" << dev << \": \" << prop.name << \" ===\\n\";\n",
        "        std::cout << \"Compute Capability: \" << prop.major << \".\" << prop.minor << \"\\n\";\n",
        "        std::cout << \"Total Global Memory: \" << prop.totalGlobalMem / (1024*1024) << \" MB\\n\";\n",
        "        std::cout << \"Multiprocessors (SMs): \" << prop.multiProcessorCount << \"\\n\";\n",
        "        std::cout << \"Max Threads per Block: \" << prop.maxThreadsPerBlock << \"\\n\";\n",
        "        std::cout << \"Max Threads per SM: \" << prop.maxThreadsPerMultiProcessor << \"\\n\";\n",
        "        std::cout << \"Warp Size: \" << prop.warpSize << \"\\n\";\n",
        "        std::cout << \"Memory Clock Rate: \" << prop.memoryClockRate / 1000 << \" MHz\\n\";\n",
        "        std::cout << \"Memory Bus Width: \" << prop.memoryBusWidth << \" bits\\n\";\n",
        "        std::cout << \"L2 Cache Size: \" << prop.l2CacheSize / 1024 << \" KB\\n\";\n",
        "\n",
        "        // Determine architecture\n",
        "        std::cout << \"\\nArchitecture: \";\n",
        "        if (prop.major == 6 && prop.minor == 0) std::cout << \"Pascal (P100)\\n\";\n",
        "        else if (prop.major == 6 && prop.minor == 1) std::cout << \"Pascal (GP10x)\\n\";\n",
        "        else if (prop.major == 7 && prop.minor == 0) std::cout << \"Volta (V100)\\n\";\n",
        "        else if (prop.major == 7 && prop.minor == 5) std::cout << \"Turing (T4/RTX 20xx)\\n\";\n",
        "        else if (prop.major == 8 && prop.minor == 0) std::cout << \"Ampere (A100)\\n\";\n",
        "        else if (prop.major == 8 && prop.minor == 6) std::cout << \"Ampere (RTX 30xx)\\n\";\n",
        "        else if (prop.major == 8 && prop.minor == 9) std::cout << \"Ada Lovelace (RTX 40xx)\\n\";\n",
        "        else if (prop.major == 9 && prop.minor == 0) std::cout << \"Hopper (H100)\\n\";\n",
        "        else std::cout << \"Unknown\\n\";\n",
        "\n",
        "        // Check for Unified Memory features\n",
        "        std::cout << \"\\nUnified Memory Features:\\n\";\n",
        "        std::cout << \"Managed Memory: \" << (prop.managedMemory ? \"Yes\" : \"No\") << \"\\n\";\n",
        "        std::cout << \"Concurrent Managed Access: \" << (prop.concurrentManagedAccess ? \"Yes\" : \"No\") << \"\\n\";\n",
        "        std::cout << \"Page Migration (Pascal+): \" << (prop.major >= 6 ? \"Yes\" : \"No\") << \"\\n\";\n",
        "    }\n",
        "\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== GPU Device 0: Tesla T4 ===\n",
            "Compute Capability: 7.5\n",
            "Total Global Memory: 15095 MB\n",
            "Multiprocessors (SMs): 40\n",
            "Max Threads per Block: 1024\n",
            "Max Threads per SM: 1024\n",
            "Warp Size: 32\n",
            "Memory Clock Rate: 5001 MHz\n",
            "Memory Bus Width: 256 bits\n",
            "L2 Cache Size: 4096 KB\n",
            "\n",
            "Architecture: Turing (T4/RTX 20xx)\n",
            "\n",
            "Unified Memory Features:\n",
            "Managed Memory: Yes\n",
            "Concurrent Managed Access: Yes\n",
            "Page Migration (Pascal+): Yes\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "nvcc -arch=sm_75 gpu_info.cu -o gpu_info\n",
        "./gpu_info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Performance Comparison: Pascal vs K80 (and newer architectures)**\n",
        "\n",
        "The question asks about Pascal GPUs vs K80. Here's why Pascal and newer GPUs perform better with Unified Memory:\n",
        "\n",
        "**K80 (Kepler, Compute 3.7):**\n",
        "- Unified Memory works but with limitations\n",
        "- No automatic page migration\n",
        "- All managed memory pages must fit in GPU memory OR be explicitly managed\n",
        "- CPU access causes GPU to wait (blocking)\n",
        "\n",
        "**Pascal (Compute 6.x) and newer:**\n",
        "- **Page Migration Engine**: Automatically migrates memory pages between CPU and GPU\n",
        "- **On-demand paging**: Pages transferred only when needed\n",
        "- **Oversubscription**: Total managed memory can exceed GPU memory\n",
        "- **Concurrent access**: CPU and GPU can access different pages simultaneously\n",
        "- **Better performance**: Reduced transfer overhead and smarter migration\n",
        "\n",
        "**Why Pascal+ is faster:**\n",
        "1. Hardware page fault handling\n",
        "2. Automatic data prefetching\n",
        "3. Better memory coherency\n",
        "4. Reduced explicit synchronization needs\n",
        "\n",
        "For GPUs with Compute Capability 6.0+, Unified Memory is significantly more efficient!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bonus: Performance Comparison with Different Memory Management\n",
        "\n",
        "Let's compare Unified Memory vs explicit memory management to see the performance difference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting compare_memory.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile compare_memory.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "#include <chrono>\n",
        "\n",
        "#define CHECK_CUDA_ERROR(call) \\\n",
        "do { \\\n",
        "  cudaError_t err = call; \\\n",
        "  if (err != cudaSuccess) { \\\n",
        "    std::cerr << \"CUDA error in \" << __FILE__ << \":\" << __LINE__ << \": \" \\\n",
        "              << cudaGetErrorString(err) << std::endl; \\\n",
        "    exit(1); \\\n",
        "  } \\\n",
        "} while(0)\n",
        "\n",
        "__global__\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "  for (int i = index; i < n; i += stride)\n",
        "    y[i] = x[i] + y[i];\n",
        "}\n",
        "\n",
        "void test_unified_memory(int N, int blockSize, int numBlocks) {\n",
        "  float *x, *y;\n",
        "\n",
        "  auto start = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "  // Allocate Unified Memory\n",
        "  CHECK_CUDA_ERROR(cudaMallocManaged(&x, N*sizeof(float)));\n",
        "  CHECK_CUDA_ERROR(cudaMallocManaged(&y, N*sizeof(float)));\n",
        "\n",
        "  // Initialize\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        "\n",
        "  // Run kernel\n",
        "  add<<<numBlocks, blockSize>>>(N, x, y);\n",
        "  CHECK_CUDA_ERROR(cudaGetLastError());\n",
        "  CHECK_CUDA_ERROR(cudaDeviceSynchronize());\n",
        "\n",
        "  auto end = std::chrono::high_resolution_clock::now();\n",
        "  std::chrono::duration<double> diff = end - start;\n",
        "\n",
        "  // Verify\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
        "\n",
        "  std::cout << \"Unified Memory - Time: \" << diff.count() << \"s, Max error: \" << maxError << std::endl;\n",
        "\n",
        "  cudaFree(x);\n",
        "  cudaFree(y);\n",
        "}\n",
        "\n",
        "void test_explicit_memory(int N, int blockSize, int numBlocks) {\n",
        "  float *x, *y;        // Host pointers\n",
        "  float *d_x, *d_y;    // Device pointers\n",
        "\n",
        "  auto start = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "  // Allocate host memory\n",
        "  x = (float*)malloc(N*sizeof(float));\n",
        "  y = (float*)malloc(N*sizeof(float));\n",
        "\n",
        "  // Initialize on host\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        "\n",
        "  // Allocate device memory\n",
        "  CHECK_CUDA_ERROR(cudaMalloc(&d_x, N*sizeof(float)));\n",
        "  CHECK_CUDA_ERROR(cudaMalloc(&d_y, N*sizeof(float)));\n",
        "\n",
        "  // Copy to device\n",
        "  CHECK_CUDA_ERROR(cudaMemcpy(d_x, x, N*sizeof(float), cudaMemcpyHostToDevice));\n",
        "  CHECK_CUDA_ERROR(cudaMemcpy(d_y, y, N*sizeof(float), cudaMemcpyHostToDevice));\n",
        "\n",
        "  // Run kernel\n",
        "  add<<<numBlocks, blockSize>>>(N, d_x, d_y);\n",
        "  CHECK_CUDA_ERROR(cudaGetLastError());\n",
        "\n",
        "  // Copy back to host\n",
        "  CHECK_CUDA_ERROR(cudaMemcpy(y, d_y, N*sizeof(float), cudaMemcpyDeviceToHost));\n",
        "  CHECK_CUDA_ERROR(cudaDeviceSynchronize());\n",
        "\n",
        "  auto end = std::chrono::high_resolution_clock::now();\n",
        "  std::chrono::duration<double> diff = end - start;\n",
        "\n",
        "  // Verify\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
        "\n",
        "  std::cout << \"Explicit Memory - Time: \" << diff.count() << \"s, Max error: \" << maxError << std::endl;\n",
        "\n",
        "  cudaFree(d_x);\n",
        "  cudaFree(d_y);\n",
        "  free(x);\n",
        "  free(y);\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  int N = 1<<20;\n",
        "  int blockSize = 256;\n",
        "  int numBlocks = (N + blockSize - 1) / blockSize;\n",
        "\n",
        "  std::cout << \"Comparing Unified Memory vs Explicit Memory Management\\n\";\n",
        "  std::cout << \"Array size: \" << N << \" elements\\n\\n\";\n",
        "\n",
        "  test_unified_memory(N, blockSize, numBlocks);\n",
        "  test_explicit_memory(N, blockSize, numBlocks);\n",
        "\n",
        "  return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Unified Memory vs Explicit Memory Management\n",
            "Array size: 1048576 elements\n",
            "\n",
            "Unified Memory - Time: 0.106914s, Max error: 0\n",
            "Explicit Memory - Time: 0.00928286s, Max error: 0\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "nvcc -arch=sm_75 compare_memory.cu -o compare_memory\n",
        "./compare_memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary of Exercise Answers\n",
        "\n",
        "### Exercise 2: Thread Execution Order\n",
        "**Question:** Do `printf()` outputs print in sequential order?\n",
        "\n",
        "**Answer:** No! The threads execute in parallel and asynchronously. The order depends on:\n",
        "- Which SM executes which block\n",
        "- Hardware scheduling decisions\n",
        "- Warp execution order\n",
        "- No guaranteed synchronization between threads\n",
        "\n",
        "This is a fundamental aspect of parallel programming - you cannot assume any execution order unless you explicitly synchronize.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 3: Multi-Dimensional Indexing\n",
        "**Question:** Why do `threadIdx.y/z`, `blockIdx.y/z`, etc. exist?\n",
        "\n",
        "**Answer:** \n",
        "- **Purpose**: Map naturally to 2D/3D problems (images, matrices, volumes)\n",
        "- **Clarity**: Makes code more readable for structured data\n",
        "- **Optimization**: Helps with memory access patterns\n",
        "\n",
        "**How to use:**\n",
        "```cpp\n",
        "dim3 blockSize(16, 16, 1);  // 256 threads\n",
        "dim3 gridSize(32, 32, 1);   // 1024 blocks\n",
        "kernel<<<gridSize, blockSize>>>(...);\n",
        "```\n",
        "\n",
        "**Defaults**: Unspecified dimensions default to 1 (for dims) or 0 (for indices)\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 4: GPU Architecture Performance\n",
        "**Question:** Is Pascal better than K80 for Unified Memory?\n",
        "\n",
        "**Answer:** Yes! Pascal (Compute 6.0+) introduced the **Page Migration Engine**:\n",
        "- Automatic page migration between CPU/GPU\n",
        "- On-demand paging (oversubscription support)\n",
        "- Hardware page fault handling\n",
        "- Better concurrent access support\n",
        "\n",
        "K80 (Kepler) has basic Unified Memory but lacks these hardware features, making it slower and less flexible.\n",
        "\n",
        "Modern GPUs (Volta, Turing, Ampere, Ada, Hopper) continue to improve on Pascal's foundation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpWVnIPujp0K"
      },
      "source": [
        "## Where to From Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTyQePjlkRJ3"
      },
      "source": [
        "If you enjoyed this notebook and want to learn more, the [NVIDIA DLI](https://nvidia.com/dli) offers several in depth CUDA Programming courses.\n",
        "\n",
        "For those of you just starting out, please consider [_Fundamentals of Accelerated Computing with CUDA C/C++_](https://courses.nvidia.com/courses/course-v1:DLI+C-AC-01+V1/about) which provides dedicated GPU resources, a more sophisticated programming environment, use of the [NVIDIA Nsight Systems™](https://developer.nvidia.com/nsight-systems) visual profiler, dozens of interactive exercises, detailed presentations, over 8 hours of material, and the ability to earn a DLI Certificate of Competency.\n",
        "\n",
        "Similarly, for Python programmers, please consider [_Fundamentals of Accelerated Computing with CUDA Python_](https://courses.nvidia.com/courses/course-v1:DLI+C-AC-02+V1/about).\n",
        "\n",
        "For more intermediate and advance CUDA programming materials, please check out the _Accelerated Computing_ section of the NVIDIA DLI [self-paced catalog](https://www.nvidia.com/en-us/training/online/)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
