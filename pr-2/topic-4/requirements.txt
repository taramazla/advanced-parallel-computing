# Requirements for LoRA Fine-tuning

# Core dependencies
torch>=2.0.0
transformers>=4.35.0
peft>=0.6.0
datasets>=2.14.0
accelerate>=0.24.0

# For quantization (QLoRA)
bitsandbytes>=0.41.0
scipy>=1.11.0

# Additional utilities
sentencepiece>=0.1.99
protobuf>=3.20.0
tqdm>=4.65.0
numpy>=1.24.0

# Evaluation
evaluate>=0.4.0
scikit-learn>=1.3.0

# Optional: for logging and monitoring
# wandb>=0.15.0
# tensorboard>=2.14.0

# Optional: for specific models
# einops>=0.7.0  # for some model architectures
# flash-attn>=2.0.0  # for flash attention (requires CUDA)
